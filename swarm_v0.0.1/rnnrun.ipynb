{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file to handle recurrent neural network part of PHLUID model\n",
    "# notice similarity to TP RNN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# paths\n",
    "MAIN_DIR = '/media/oscar47/Oscar Extra/Physics data/swarm_data'\n",
    "DATA_DIR= os.path.join(MAIN_DIR, 'rnn_output')\n",
    "MAX_FRAME = 120 # number of consecutive frame fv groupings\n",
    "\n",
    "# load datasets\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'train_x.npy'))\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'train_y.npy'))\n",
    "x_val = np.load(os.path.join(DATA_DIR, 'val_x.npy'))\n",
    "y_val = np.load(os.path.join(DATA_DIR, 'val_y.npy'))\n",
    "\n",
    "# build model functions--------------------------------\n",
    "def train_custom(LSTM_layer_size_1=128,  LSTM_layer_size_2=128, LSTM_layer_size_3=128, \n",
    "              LSTM_layer_size_4=128, LSTM_layer_size_5=128, \n",
    "              dropout=0.1, learning_rate=0.01, epochs=1, batchsize=32):\n",
    "    #initialize the neural net; \n",
    "    global model\n",
    "    model = build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "            LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "            dropout, learning_rate)\n",
    "    \n",
    "    #now run training\n",
    "    history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size = batchsize,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "    )\n",
    "\n",
    "# define model\n",
    "def build_model(LSTM_layer_size_1,  LSTM_layer_size_2, LSTM_layer_size_3, \n",
    "          LSTM_layer_size_4, LSTM_layer_size_5, \n",
    "          dropout, learning_rate):\n",
    "    # call initialize function\n",
    "    \n",
    "    model = Sequential()\n",
    "    # RNN layers for language processing\n",
    "    model.add(LSTM(LSTM_layer_size_1, input_shape = (x_train[0].shape), return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_2, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_3, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_4, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layer_size_5))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(len(y_train[0])))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    # put structure together\n",
    "    optimizer = RMSprop(learning_rate = learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_custom_resume(model, batchsize, epochs):\n",
    "   #now run training\n",
    "   history = model.fit(\n",
    "   x_train, y_train,\n",
    "   batch_size = batchsize,\n",
    "   validation_data=(x_val, y_val),\n",
    "   epochs=epochs,\n",
    "   callbacks=callbacks #use callbacks to have w&b log stats; will automatically save best model                     \n",
    "   )\n",
    "\n",
    "# define two other callbacks\n",
    "# save model\n",
    "# if no directory \"models\" exists, create it\n",
    "if not(os.path.exists('models2')):\n",
    "    os.mkdir('./models2/')\n",
    "modelpath = \"models2/shakespeare_v0.0.1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')\n",
    "# if learning stals, reduce the LR\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "\n",
    "# compile the callbacks\n",
    "#callbacks = [checkpoint, reduce_lr, WandbCallback()]\n",
    "callbacks = [checkpoint, reduce_lr]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
